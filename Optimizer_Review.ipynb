{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <u><center>Hyper-Parameter optimization of Neural-Network-Models in algorithmic trading environments: a Python-based approach</center></u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Review-Version, January 2023, Dr. Andreas Horzella***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increasing impact of artificial intelligence (AI) applications on more and more aspects of life is obvious. Consequently, the financial sector has also been heavily involved in corresponding developments in recent years. In terms of trading financial instruments, neural networks and reinforcement learning (RL) agents are promising approaches for building an alpha-generating algorithmic trading system: experience and gut feeling of a human trader are replaced by a complex network of calculated input-output relations. These are multi-dimensional, multi-layered and generated from existing and emerging market data using Machine Learning (ML) algorithms. The result are models that can provide statistically based forecasts of future market developments.\n",
    "\n",
    "In addition to the availability of high-quality data, the calibration of the model parameters represents a main factor influencing the forecast quality. This includes the design of the model itself, performed by the training process, as well as the adjustment of the hyper parameters in advance, both a mutual and ongoing process in a fast-moving financial market environment.\n",
    "\n",
    "When it comes to the design of the hyper parameters, it is the responsibility of the developer to thoroughly orchestrate the interaction of all variables. The variety of possibilities here is huge and sometimes it is quite challenging and time consuming to come up with an appropriate solution. In fact, this configuration process represents one major, time-consuming effort while building a reliable forecast model. Hence it is crucial to have supporting tools at hand which reduce the vast variety of possible configurations to the most promising ones and thereby dramatically boil down the development time for new AI models.\n",
    "\n",
    "Tools for hyper-parameter-optimization / hyper-tuning already do exist, but basically focus merely on the AI Model instead of optimizing the whole environment: prerequisite for optimizers like “Keras Tuner” is an already specified Model and a defined data- and feature set. Thus, the necessity for a dedicated, comprehensive tuning system for the design of AI-based algorithmic trading system is obvious. The purpose of this paper is to build a framework to facilitate the design of AI models for algorithmic trading systems in an automated manner, considering both model-/algorithmic parameters as well as data- and featureset related modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach, Methods, Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization approach consists of a Jupyter notebook, including two classes for data handling and model building. Those classes make it easier to streamline the following process and are additionally reusable for other cases.\n",
    "\n",
    "First, the base parameters for the optimization and the tuning modes are set. Among those are the financial instrument, the granularity, timeframe and featureset. For demonstration purposes, a exemplary setup for the required parameters is included within the external configuration file \"algo_config.cfg\", which is read by using the configparser functionality. This file can be used to easily create user-specific scenarios.\n",
    "\n",
    "The tuning mode defines which mathematical-statistical model shall be used for solving the optimization problem. The Search Spectrum, on the other hand, contains the determinants of the actual sequential neural network itself, in terms of number of units, number of layers, optimizer etc. Also these are specified in the configuration file.\n",
    "\n",
    "As soon as data- and analysis-frameworks are defined, the main optimization loop initiates the tuning process by applying the selected tuning model to the defined data and search spectrum. At the end, the process provides an optimized model with the best hyperparameters and model variables which can be saved for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different approaches to implement hyper parameter optimization for a keras-based neural network. Some of the most popular tools are:\n",
    "\n",
    "- Keras Optimizer\n",
    "- Scikit-Optimize\n",
    "- Hyperopt\n",
    "- Ray Tune\n",
    "- Optuna\n",
    "- Talos\n",
    "- Spearmint\n",
    "\n",
    "For the selection of the \"right\" tool, one should consider criteria like keras/tensorflow integration, performance and preferably low demand for adaptation of existing code. A good compromise in terms of complexity, usability and scalability is KERAS TUNER which is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the imports for the whole project require standard libraries like numpy, pandas and pylab. Additionally, the tpqoa module from The Python Quants for data import from OANDA data provider. Nevertheless, the exemplary process in this document will be based on fixed datafiles, containing minutely EUR-USD exchange rate for the timeframe 2021 January - 2022 September. The minutely data will then be resampled to hourly data. It is recommended to use a higher frequency for download from provider than afterwards used for analysis, since provider data sometimes may contain gaps which can be filled on a lower frequency level using the higher frequency source. Finally, tensorflow as well as keras and sklearn are imported, together with the keras tuner package which is the essential module for this thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for Google Colab Environment\n",
    "\n",
    "# Install Keras Tuner\n",
    "%pip install keras_tuner\n",
    "\n",
    "# Prepare GOOGLE COLAB execution\n",
    "!git clone https://github.com/AHorzella/tpq_algo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt, mpl\n",
    "import configparser\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Modules to be installed via \"pip install keras-tuner\":\n",
    "import keras_tuner                              \n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ignore performance warning from pandas due to excessive pd dataframe handling\n",
    "from warnings import simplefilter               \n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Necessary to clear output from a specific jupyter notebook cell\n",
    "from IPython.display import clear_output        \n",
    "\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
    "\n",
    "# Load basic configfile with important parameters for tuning process\n",
    "model_config = configparser.ConfigParser()\n",
    "\n",
    "# Activate one of the following versions A/B (Google Colab vs. local execution)\n",
    "\n",
    "# VERSION A: execute from Google Colab\n",
    "model_config.read('tpq_algo/algo_config.cfg')\n",
    "sys.path.append(\"tpq_algo\")\n",
    "\n",
    "# VERSION B: execute local\n",
    "#model_config.read('algo_config.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PARAMETERS: DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database parameters include symbol, price type, start- and enddate and granularity as well as the featureset. All parameters are read from the config file \"algo.config.cfg\". This enables the creation of different, regularly used scenarios.\n",
    "\n",
    " Current features are: return, simple moving average, minimum- and maximum-values, momentum, volatility and average true range indicator. Additional features may be added by the user. \n",
    " \n",
    " Furthermore, it can be defined:\n",
    "- norm_select: use normalized data yes/no\n",
    "- test_share and validation_share: divide dataset into train-, test- and validation-data\n",
    "- lags: number of past periods to be used for neural network prognosis\n",
    "- window: number of periods for feature calculation (e.g. moving average)\n",
    "- tuning_mode and tuning_spectrum: scenario selection for tuning (currently 1, 2 or 3 each; details see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from configfile\n",
    "\n",
    "configpath = model_config['general']['configpath']\n",
    "symbol =  model_config['database']['symbol'].split()\n",
    "\n",
    "price = model_config['database']['price']\n",
    "startdate = model_config['database']['startdate']\n",
    "enddate = model_config['database']['enddate']\n",
    "api_granularity = model_config['database']['api_granularity']\n",
    "granularity = model_config['database']['granularity']\n",
    "\n",
    "norm_select = bool(model_config['database']['norm_select'])\n",
    "test_share = float(model_config['database']['test_share'])\n",
    "validation_share = float(model_config['database']['validation_share'])\n",
    "\n",
    "featureset = model_config['database']['featureset'].split()\n",
    "lags = json.loads(model_config['database']['lags'])\n",
    "windows = json.loads(model_config['database']['windows'])\n",
    "\n",
    "tuning_mode = int(model_config['tuningmode']['tuning_mode'])\n",
    "tuning_spectrum = int(model_config['tuningmode']['tuning_spectrum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETERS: KERAS TUNER TUNING MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The KERAS Tuner parameters are divided into \"Keras Tuner Mode\" and \"Keras Tuner Spectrum\". To keep the model user friendly, both categories are wrapped into three typical scenarios each, which can be selected upfront via the variables \"tuning_mode\" and \"tuning_spectrum\". In simplest case, the user just selects the scenarios und works with them, or he can get into detail and modify the scenarios directly. The scenarios are designed to provide three typical tuning environments, from simple (variables=1) to complex (variables=3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Tuner offers three basic tuner classes for optimization. The selection of the tuning mode offers the corresponding choice between \"Random Search\", \"Bayesian Optimization\" and \"Hyperband\". All of them are based on the so-called \"Grid\", which contains all possible combinations of hyperparameters in a model. The Grid is n-dimensional with n equals the number of hyperparameters to be tuned. To test all those combinations in reality would not be efficient, and in most cases not feasible due to the required huge calculation power. Instead, the algorithms choose intelligently subsets of the Grid and incrementally follow a defined path of optimization. Main characteristics of each class are: \n",
    "\n",
    "- Random Search: from the \"Grid\" with all hyperparameter combinations, in each iteration a random subset is chosen and the result of the loss function is calculated to estimate the model performance. At the end, the combination of the best hyperparameters is returned. Note: by far not all possible combinations are tested, only as many iterations as defined in the setup.\n",
    "- Bayesian Optimization: this algorithm focuses on the minimization problem of the loss function. In the contrary to the random search, the selection of the hyperparameters for the next step is not randomly but derived from the likelihood to get a better parameter combination compared to the previous step.\n",
    "- Hyperband: this tuning class works similar to random search but does not apply full training cycles to obviously bad combinations of hyperparameters. To achieve this, the method is based on the explore-exploit concept. This means, in the first step promising hyperparameter combinations are gained by training those on a comparably low number of epochs. The best-performing candidates then are passed to the main cycle with the time-consuming calculation over all defined epochs. This significantly avoids wasting time and computing power.\n",
    "\n",
    "Basically, each tuner class offers individual variables for customization, e.g. number of epochs, objective or other specific factors. These variables can be modified below in each tuner-class-related scenario. By default, they are set to a robust value and can be taken for the first steps as they are. If desired, those variables could also been transferred to the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all tuning modes, the accuracy on the validation dataset is target\n",
    "\n",
    "if tuning_mode == 1:\n",
    "    ktuner = 'RandomSearch'\n",
    "    t_search_epochs = 10\n",
    "    t_model_fit_epochs = 10\n",
    "    t_objective='val_accuracy'      \n",
    "    r_max_trials = 10\n",
    "    r_executions_per_trial = 2\n",
    "\n",
    "if tuning_mode == 2:\n",
    "    ktuner = 'BayesianOptimization'\n",
    "    t_search_epochs = 10\n",
    "    t_model_fit_epochs = 10\n",
    "    t_objective = 'val_accuracy'\n",
    "    b_max_trials = 5\n",
    "    b_num_initial_points = 2\n",
    "    b_alpha = 0.0001\n",
    "    b_beta = 2.6\n",
    "\n",
    "if tuning_mode == 3:\n",
    "    ktuner = 'Hyperband'\n",
    "    t_search_epochs = 10\n",
    "    t_model_fit_epochs = 10\n",
    "    t_objective='val_accuracy'\n",
    "    h_max_epochs = 5\n",
    "    h_factor = 3\n",
    "    h_iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETERS: KERAS TUNER SEARCH SPECTRUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selection of the tuning class, several parameters regarding the neural network model itself have to be customized. Those are:\n",
    "- number of layers of the model\n",
    "- number of units of the model\n",
    "- activation function\n",
    "- learning rate\n",
    "- dropout mode\n",
    "- optimizer\n",
    "- regulation mode\n",
    "- loss function\n",
    "\n",
    "Again, all listed variables are wrapped in three scenarios that either can be chosen one to one or individually modified. From 1 to 3, the scenarios get more complex and time consuming during calculation but should also get more reliable. For example, whereas in scenario 1 only a maximum of 3 layers will be used, in scenario 3 the number of layers can go up to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuning_spectrum == 1:\n",
    "    hidden_layers_min_val = 1\n",
    "    hidden_layers_max_val = 3\n",
    "    hidden_units_min_val = 32\n",
    "    hidden_units_max_val = 256\n",
    "    hidden_units_step_val = 32\n",
    "    activation_val = ['relu']\n",
    "    dropout_rate_val = [0.3]\n",
    "    lr_min_val = 1e-4\n",
    "    lr_max_val = 1e-2\n",
    "    lr_sampling_val = 'log'\n",
    "    optimizer_val =['rmsprop']\n",
    "    reg_value_val = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_val = ['l1', 'l2']\n",
    "    loss_funct_val = ['binary_crossentropy']\n",
    "\n",
    "if tuning_spectrum == 2:\n",
    "    hidden_layers_min_val = 1\n",
    "    hidden_layers_max_val = 5\n",
    "    hidden_units_min_val = 32\n",
    "    hidden_units_max_val = 512\n",
    "    hidden_units_step_val = 32\n",
    "    activation_val = [\n",
    "        'relu',\n",
    "        'sigmoid',\n",
    "        'linear'\n",
    "        ]\n",
    "    dropout_rate_val = [0.0, 0.3, 0.5]\n",
    "    lr_min_val = 1e-4\n",
    "    lr_max_val = 1e-2\n",
    "    lr_sampling_val = 'log'\n",
    "    optimizer_val =[\n",
    "        'rmsprop',\n",
    "        'adam'\n",
    "        ]\n",
    "    reg_value_val = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_val = ['l1', 'l2']\n",
    "    loss_funct_val = [\n",
    "        'binary_crossentropy',\n",
    "        'mean_squared_error'\n",
    "        ]\n",
    "\n",
    "if tuning_spectrum == 3:\n",
    "    hidden_layers_min_val = 1\n",
    "    hidden_layers_max_val = 7\n",
    "    hidden_units_min_val = 32\n",
    "    hidden_units_max_val = 1024\n",
    "    hidden_units_step_val = 32\n",
    "    activation_val = [\n",
    "        'relu',\n",
    "        'tanh',\n",
    "        'sigmoid',\n",
    "        'softmax',\n",
    "        'softplus', \n",
    "        'softsign',\n",
    "        'selu',\n",
    "        'elu',\n",
    "        'exponential',\n",
    "        'linear'\n",
    "        ]\n",
    "    dropout_rate_val = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    lr_min_val = 1e-4\n",
    "    lr_max_val = 1e-2\n",
    "    lr_sampling_val = 'log'\n",
    "    optimizer_val = [\n",
    "        'sgd',\n",
    "        'rmsprop',\n",
    "        'adagrad',\n",
    "        'adadelta',\n",
    "        'adam',\n",
    "        'nadam'\n",
    "        ]\n",
    "    reg_value_val = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_val = ['l1', 'l2']\n",
    "    loss_funct_val = [\n",
    "        'binary_crossentropy',\n",
    "        'mean_squared_error',\n",
    "        'mean_squared_logarithmic_error',\n",
    "        'mean_absolute_error',\n",
    "        'kl_divergence'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional functions are required to initialize the seed and to apply weighting to the input dataset. Without weighting, the model probably will return biased output when used for imbalanced datasets. Both classes are taken from \"The Python Quants\" repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "def cw(df):\n",
    "    c0, c1 = np.bincount(df['d'])\n",
    "    w0 = (1 / c0) * (len(df)) / 2\n",
    "    w1 = (1 / c1) * (len(df)) / 2\n",
    "    return {0: w0, 1: w1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS FOR DATA MANAGEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class DataEnvironment provides the basic functionality to load required financial instrument data. Once called, the \"init\"-procedure will load the defined data and prepare it for further processing. Following steps will be conducted:\n",
    "- _get_data: required data is loaded local file.\n",
    "For the purpose of this paper, a predefined file is used in order to guarantee replicability.\n",
    "- _add_features: this function calculates the features based on that the AI algorithm will build the neural network model. From the pre-defined features a selection can be done via the featureset array in config file.\n",
    "- _add_lags: the neural network model will do its forecast based on historical data. The number of lags defines the number of periods to look backwards. For all features this historical data will here be provided in the desired structure.\n",
    "- _data_split: building a neural network model means training, validating and testing the model. The existing dataset will be separated into three parts to enable this procedure.\n",
    "- _normalize: normalized data generally leads to better results in training a neural network. This function applies a mu/sigma-standardization to the dataset.\n",
    "\n",
    "Additinally, two functions are added to gain details of the dataset and plot the data.\n",
    "\n",
    "The whole class DataEnvironment is part of the external file \"data_environment.py\" and will just be imported here for further processings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_environment as denv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the DataEnvironment class, it can be initialized with the desired parameters. This generates the analysis datasets. Then data details can be shown and price data will be plotted in a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir = denv.DataEnvironment(\n",
    "    symbol=symbol[0],\n",
    "    api_granularity=api_granularity,\n",
    "    granularity=granularity,\n",
    "    window=20,\n",
    "    lags=5,\n",
    "    validation_share=validation_share,\n",
    "    test_share=test_share,\n",
    "    norm_select=norm_select,\n",
    "    featureset=featureset,\n",
    "    startdate=startdate,\n",
    "    enddate=enddate,\n",
    "    price=price\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show details of unprocessed raw data\n",
    "envir.raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unprocessed raw data in table format\n",
    "envir.raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resampled price data\n",
    "envir.plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS FOR MODEL BUILDING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been created, KERAS Tuner needs a Hypermodel as prototype to be optimized. Same as for DataEnvironment, the Hypermodel is also wrapped in a class module and provides following functions:\n",
    "- create_model: this is the main function to create the neural networks based on the received input. Since KERAS Tuner has to evaluate many different neural networks to finally come up with the best solution, this function has to be flexible in terms of varying the input values. To create the best model means to tune exactly those input parameters within given ranges of freedom. The create_model function on its part receives the allowed input ranges from the build function.\n",
    "- build: the main task of the build function is to create the optimization ranges for the hyperparameters and pass them to the create_model function for further processing. Most important here is the \"hp\"-function which easily allows to define search spaces for the tuning algorithm.\n",
    "- opt_wrapper / reg_wrapper: helper functions to create parameter ranges for optimizer and regularization.\n",
    "- fit: create neural network model based on the selected hyper parameters\n",
    "\n",
    "The whole class HyperModel is part of the external file \"hypermodel.py\" and will just be imported here for further processings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypermodel as hypm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN OPTIMIZATION LOOP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop represents the core code for the KERAS Tuner approach, orchestrating amongst others the model_create- and build-procedures. It basically iterates over possible (hyper-) parameter combinations in order to find the best solution that leads to the minimal value of the loss function which is the core of the neural network optimization process. \n",
    "\n",
    "In each loop, the DataEnvironment Class creates the dataset according to the given parameters. Then the Keras Tuner is initialized and the tuner.search procedure is started. This is the most time-consuming part of the tuning process and can take, depending on the complexity of the tuning setup, between some minutes and some hours. Each valid parameter combination is saved to the results table. At the end, the summary()-function shows details like layers, shapes and number of parameters for the best model.\n",
    "\n",
    "Important note: a model is classified to be \"valid\", if the directions predicted are not only 100% in one direction. In a falling market for example a model would be successful, if it consequently forecasts short positions only. Such a model would loose all if the market turns up. Therefore, the target must be to train a model which is feasible for different market situations over time. And this is only the case if the model does predict ups as well as downs. Additionally, the accuracy of a valid model must be above the 50%-level, since this value represents the random choice approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ac_mem = 0\n",
    "tseed = 1\n",
    "\n",
    "for symbol_sel in symbol:\n",
    "\n",
    "    for l in lags:\n",
    "\n",
    "        for w in windows:\n",
    "\n",
    "            status = False\n",
    "            \n",
    "            envir = denv.DataEnvironment(\n",
    "                symbol=symbol_sel,\n",
    "                api_granularity=api_granularity,\n",
    "                granularity=granularity,\n",
    "                window=w,\n",
    "                lags=l,\n",
    "                validation_share=validation_share,\n",
    "                test_share=test_share,\n",
    "                norm_select=norm_select,\n",
    "                featureset=featureset,\n",
    "                startdate=startdate,\n",
    "                enddate=enddate,\n",
    "                price=price\n",
    "                )\n",
    "\n",
    "            if ktuner == 'RandomSearch':\n",
    "\n",
    "                tuner = keras_tuner.RandomSearch(\n",
    "                    hypermodel=hypm.TheHyperModel(envir, hidden_layers_min_val, hidden_layers_max_val,\n",
    "                                                  hidden_units_min_val, hidden_units_max_val,\n",
    "                                                  hidden_units_step_val, activation_val,\n",
    "                                                  dropout_rate_val, lr_min_val, lr_max_val,\n",
    "                                                  lr_sampling_val, optimizer_val,\n",
    "                                                  reg_value_val, reg_val, loss_funct_val),\n",
    "                    objective=t_objective,\n",
    "                    max_trials=r_max_trials,\n",
    "                    executions_per_trial=r_executions_per_trial,\n",
    "                    seed=tseed,\n",
    "                    hyperparameters=None,\n",
    "                    tune_new_entries=True,\n",
    "                    allow_new_entries=True,\n",
    "                    overwrite = True,\n",
    "                    directory='Datastore/my_dir',\n",
    "                    project_name='tune_hypermodel'\n",
    "                    )\n",
    "\n",
    "            if ktuner == 'BayesianOptimization':\n",
    "\n",
    "                tuner = keras_tuner.BayesianOptimization(\n",
    "                    hypermodel=hypm.TheHyperModel(envir, hidden_layers_min_val, hidden_layers_max_val,\n",
    "                                                  hidden_units_min_val, hidden_units_max_val,\n",
    "                                                  hidden_units_step_val, activation_val,\n",
    "                                                  dropout_rate_val, lr_min_val, lr_max_val,\n",
    "                                                  lr_sampling_val, optimizer_val,\n",
    "                                                  reg_value_val, reg_val, loss_funct_val),\n",
    "                    objective=t_objective,\n",
    "                    max_trials=b_max_trials,\n",
    "                    num_initial_points=b_num_initial_points,\n",
    "                    alpha=b_alpha,\n",
    "                    beta=b_beta,\n",
    "                    seed=tseed,\n",
    "                    hyperparameters=None,\n",
    "                    tune_new_entries=True,\n",
    "                    allow_new_entries=True,\n",
    "                    overwrite = True,\n",
    "                    directory='Datastore/my_dir',\n",
    "                    project_name='tune_hypermodel'\n",
    "                    )\n",
    "            \n",
    "            if ktuner == 'Hyperband':\n",
    "\n",
    "                tuner = keras_tuner.Hyperband(\n",
    "                    hypermodel=hypm.TheHyperModel(envir, hidden_layers_min_val, hidden_layers_max_val,\n",
    "                                                  hidden_units_min_val, hidden_units_max_val,\n",
    "                                                  hidden_units_step_val, activation_val,\n",
    "                                                  dropout_rate_val, lr_min_val, lr_max_val,\n",
    "                                                  lr_sampling_val, optimizer_val,\n",
    "                                                  reg_value_val, reg_val, loss_funct_val),\n",
    "                    objective=t_objective,\n",
    "                    max_epochs=h_max_epochs,\n",
    "                    factor=h_factor,\n",
    "                    hyperband_iterations=h_iterations,\n",
    "                    seed=tseed,\n",
    "                    hyperparameters=None,\n",
    "                    tune_new_entries=True,\n",
    "                    allow_new_entries=True,\n",
    "                    overwrite = True,\n",
    "                    directory='Datastore/my_dir',\n",
    "                    project_name='tune_hypermodel'\n",
    "                    )\n",
    "\n",
    "            # Based on the environment defined above, the tuning process is started                        \n",
    "            tuner.search(\n",
    "                envir.train_[envir.cols],\n",
    "                envir.train['d'],\n",
    "                epochs=t_search_epochs,\n",
    "                validation_data=(envir.validation_[envir.cols], envir.validation['d']),\n",
    "                class_weight=cw(envir.train),\n",
    "                verbose = False\n",
    "                )\n",
    "            \n",
    "            # Retrain the model with best hyperparameters on combined train+validation dataset\n",
    "            hypermodel=hypm.TheHyperModel(envir, hidden_layers_min_val, hidden_layers_max_val,\n",
    "                                                  hidden_units_min_val, hidden_units_max_val,\n",
    "                                                  hidden_units_step_val, activation_val,\n",
    "                                                  dropout_rate_val, lr_min_val, lr_max_val,\n",
    "                                                  lr_sampling_val, optimizer_val,\n",
    "                                                  reg_value_val, reg_val, loss_funct_val)\n",
    "            best_hp = tuner.get_best_hyperparameters()[0]\n",
    "            model = hypermodel.build(best_hp)\n",
    "            model_mem = model\n",
    "            hist = hypermodel.fit(best_hp, model, envir.trainval[envir.cols], envir.trainval['d'],\n",
    "                                  epochs=t_model_fit_epochs, class_weight=cw(envir.trainval))\n",
    "\n",
    "            # Get model performance (accuracy)\n",
    "            ls, ac = model.evaluate(envir.trainval[envir.cols], envir.trainval['d'])\n",
    "\n",
    "            # Use current model for prediction\n",
    "            envir.trainval['predresult'] = model.predict(envir.trainval[envir.cols])\n",
    "            envir.trainval['pred'] = np.where(model.predict(envir.trainval[envir.cols]) > 0.5, 1, 0)\n",
    "            directions = envir.trainval['pred'].value_counts().index.values\n",
    "            preds = envir.trainval['pred'].value_counts().values\n",
    "\n",
    "            # Calculate \"real\" result\n",
    "            perform = sum(envir.trainval['r']*envir.trainval['pred'])/envir.trainval[symbol_sel][0]*100\n",
    "            \n",
    "            # Calulate ratio between 0 and 1 predictions\n",
    "            if directions.size > 1:\n",
    "                up = envir.trainval['pred'].value_counts()[1]\n",
    "                down = envir.trainval['pred'].value_counts()[0]\n",
    "                ratio = min(up, down)/max(up, down)\n",
    "            \n",
    "            # If valid solution: predictions in both directions and accuracy >50% and ratio 0/1 not closwe to 0\n",
    "            if directions.size > 1 and ac > 0.5 and ratio>0.1:\n",
    "                \n",
    "                status = True\n",
    "            \n",
    "                # Append valid results to dataframe\n",
    "                results.append([status, symbol_sel, startdate, enddate, granularity,\n",
    "                                perform, ac, directions, preds, ratio, featureset, l, w,\n",
    "                                tuner.get_best_hyperparameters()[0]['layers'],\n",
    "                                tuner.get_best_hyperparameters()[0]['units'],\n",
    "                                tuner.get_best_hyperparameters()[0]['activation'],\n",
    "                                tuner.get_best_hyperparameters()[0]['dropout'],\n",
    "                                tuner.get_best_hyperparameters()[0]['drop_rate'],\n",
    "                                tuner.get_best_hyperparameters()[0]['lr'],\n",
    "                                tuner.get_best_hyperparameters()[0]['optimizer'],\n",
    "                                tuner.get_best_hyperparameters()[0]['regularize'],\n",
    "                                tuner.get_best_hyperparameters()[0]['reg_value'],\n",
    "                                tuner.get_best_hyperparameters()[0]['type'],\n",
    "                                tuner.get_best_hyperparameters()[0]['loss_funct']]\n",
    "                                )\n",
    "                \n",
    "                if ac > ac_mem:                 # If current accuracy is greater than all accuracies so far ...\n",
    "                    ac_mem = ac                 # ... save current accuracy as new best value ...\n",
    "                    model_best = model_mem      # ... and save corresponding model as new best model\n",
    "\n",
    "# Results table now is formatted as pd dataframe with appropriate column naming\n",
    "finres = pd.DataFrame(\n",
    "    results, columns=[\n",
    "        'Valid', 'Symbol', 'Start',\n",
    "        'End', 'Granularity', 'Performance',\n",
    "        'Accuracy', 'Directions', 'Move',\n",
    "        'Ratio', 'Featureset', 'Lags',\n",
    "        'Window', 'Layers', 'Units',\n",
    "        'Activation', 'Dropout', 'DropoutRate',\n",
    "        'LR', 'Optimizer', 'Regularize',\n",
    "        'RegValue', 'RegType', 'LossFunct']\n",
    "        )\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "# If at least one valid solution was found, print structure of best model\n",
    "if len(results) >= 1:\n",
    "    print(model_best.summary())\n",
    "else:\n",
    "    print('No valid solution found. Please change settings and restart.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning process, the results are shown in table format. First, all (valid) tuned models, and then the best model with its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all valid models: main parameters in DataFrame-Format\n",
    "finres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for the best model from whole dataframe and show them in Dataframe-Format as well\n",
    "best_params = finres.loc[finres['Accuracy'] == max(finres['Accuracy'])]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the best model including its dataset will be rebuild and evaluated. This is necessary because the best model is not necessarily the last one which has been calculated during tuning process and which still remains in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir_t = denv.DataEnvironment(\n",
    "    symbol=best_params['Symbol'].values[0],\n",
    "    api_granularity=api_granularity,\n",
    "    granularity=granularity,\n",
    "    window=int(best_params['Window']),\n",
    "    lags=int(best_params['Lags']),\n",
    "    validation_share=validation_share,\n",
    "    test_share=test_share,\n",
    "    norm_select=norm_select,\n",
    "    featureset=best_params['Featureset'].values[0],\n",
    "    startdate=startdate,\n",
    "    enddate=enddate,\n",
    "    price=price\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model (according to Keras Tuner) with specific datasets (in terms of features, lags and window)\n",
    "\n",
    "if int(envir_t.train.size) > 0:\n",
    "    print('Evaluation on train dataset:')\n",
    "    model_best.evaluate(envir_t.train[envir_t.cols], envir_t.train['d'])\n",
    "if int(envir_t.validation.size) > 0:\n",
    "    print('Evaluation on validation dataset:')\n",
    "    model_best.evaluate(envir_t.validation[envir_t.cols], envir_t.validation['d'])\n",
    "if int(envir_t.trainval.size) > 0:\n",
    "    print('Evaluation on combined train-/validation-dataset:')\n",
    "    model_best.evaluate(envir_t.trainval[envir_t.cols], envir_t.trainval['d'])\n",
    "if int(envir_t.test.size) > 0:\n",
    "    print('Evaluation on test dataset:')\n",
    "    model_best.evaluate(envir_t.test[envir_t.cols], envir_t.test['d'])\n",
    "if int(envir_t.data.size) > 0:\n",
    "    print('Evaluation on whole data dataset:')\n",
    "    model_best.evaluate(envir_t.data[envir_t.cols], envir_t.data['d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is applied to the whole dataset for prediction, the number of upward-predictions (1) and neutral/downward-predictions (0) can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir_t.data['predresult'] = model_best.predict(envir_t.data[envir_t.cols])\n",
    "envir_t.data['pred'] = np.where(model_best.predict(envir_t.data[envir_t.cols]) > 0.5, 1, 0)\n",
    "envir_t.data['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it is shown how the final model can be applied to any other timeframe just by creating a new database instance and performing the model evaluation on this data. Even those results are promising since accuracy is still above 60%. This is remarkable since 2020 shows a bullish market, whereas the data for model training (2022) comes from a bearish market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = \"2020-01-01\"\n",
    "enddate = \"2021-01-01\"\n",
    "granularity = '1h'\n",
    "\n",
    "envir_t = denv.DataEnvironment(\n",
    "    symbol=best_params['Symbol'].values[0],\n",
    "    api_granularity=api_granularity,\n",
    "    granularity=granularity,\n",
    "    window=int(best_params['Window']),\n",
    "    lags=int(best_params['Lags']),\n",
    "    validation_share=validation_share,\n",
    "    test_share=test_share,\n",
    "    norm_select=norm_select,\n",
    "    featureset=best_params['Featureset'].values[0],\n",
    "    startdate=startdate,\n",
    "    enddate=enddate,\n",
    "    price=price\n",
    "    )\n",
    "\n",
    "envir_t.plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluation on whole data dataset:')\n",
    "model_best.evaluate(envir_t.data[envir_t.cols], envir_t.data['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir_t.data['predresult'] = model_best.predict(envir_t.data[envir_t.cols])\n",
    "envir_t.data['pred'] = np.where(model_best.predict(envir_t.data[envir_t.cols]) > 0.5, 1, 0)\n",
    "envir_t.data['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SAVE TUNER AND MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tuning process typically is time consuming, the results should be saved for further processing. The model itself may be used for a live trading environment, the tuner setup may be required for analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This works only locally\n",
    "# with open(f\"Datastore/tuner_save.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(tuner, f)\n",
    "\n",
    "model_best.save('Datastore/BestModel')\n",
    "best_params.to_pickle('Datastore/bestparams.pkl')\n",
    "\n",
    "# Save result history\n",
    "try:\n",
    "    resulthistory = pd.read_csv('Datastore/results.csv', index_col=0)\n",
    "    newresulthistory = pd.concat([resulthistory, finres])\n",
    "except:\n",
    "    newresulthistory = finres\n",
    "newresulthistory.to_csv('Datastore/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD TUNER AND MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completeness, the following code shows how to reload all data to the system after a reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tuner object; all modules must be executed before;\n",
    "# Pickle doesn't store info about how a class/object is constructed,\n",
    "# and needs access to the class when unpickling\n",
    "#tuner = pickle.load(open('Datastore/tuner_save.pkl', 'rb'))\n",
    "best_params = pd.read_pickle('Datastore/bestparams.pkl')\n",
    "model_best = keras.models.load_model('Datastore/BestModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of KERAS TUNER to a neural network optimization problem relieves burden from the developer significantly by heavily decreasing development time and calculation effort. The results consistently show accuracy rates above 60% for the example shown above, focusing on the EUR-USD exchange rate for the years 2021 and 2022.\n",
    "\n",
    "In this approach, two main classes provide reusability and increase ease of use. These may be extracted to an external script to be imported for use.\n",
    "\n",
    "Possible Extension:\n",
    "- To further improve results, application of bagging could be one promising approach.\n",
    "- Data like volume- and orderbook-information may improve the predictions.\n",
    "- The prediction currently separates only between the two options up- and neutral/down. Prediction of all three options (up, down, neutral) may be implemented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed892ed972b8c5d1fb85e6442f7bcc25a1f99543fd0b88210cd4b3fd2128b858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
